# -*- coding: utf-8 -*-
"""Scenario_Refiner_LXMERT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L0eq3pi7ZxmHl09BG88l_DpdCM3Gc-9G
"""

!pip install transformers

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/huggingface/transformers
# %cd transformers
# %cd examples/research_projects/lxmert
!pip install wget

from transformers import LxmertTokenizer, LxmertForPreTraining, LxmertConfig
import torch
import os
import glob
import cv2
#import matplotlib.pyplot as plt
torch.cuda.is_available()

tokenizer = LxmertTokenizer.from_pretrained("unc-nlp/lxmert-base-uncased")
#config = LxmertConfig.from_pretrained("unc-nlp/lxmert-base-uncased")
#config.task_matched = True
#config.task_obj_predict= True
#config.mask_lm = False
#config.task_qa= False
model = LxmertForPreTraining.from_pretrained("unc-nlp/lxmert-base-uncased")

import numpy as np
from utils import Config
from processing_image import Preprocess
from visualizing_image import SingleImageViz
from modeling_frcnn import GeneralizedRCNN

frcnn_cfg = Config.from_pretrained("unc-nlp/frcnn-vg-finetuned")
frcnn = GeneralizedRCNN.from_pretrained("unc-nlp/frcnn-vg-finetuned", config=frcnn_cfg)
image_preprocess = Preprocess(frcnn_cfg)

import numpy as np

text_bake = ["The woman is a baker",
         "The woman is baking",
         "The man is a baker",
         "The man is baking",
         "The person is a baker",
         "The person is baking"]

text_surf = ["The man is a surfer",
         "The man is surfing",
         "The woman is a surfer",
         "The woman is surfing",
         "The woman and the man are surfers",
         "The woman and the man are surfing",
         "The man and the young girl are surfers",
         "The man and the young girl are surfing",
         "The four kids are surfers",
         "The four kids are surfing"]

text_hunt = ["The four men are hunters",
         "The four men are hunting",
         "The cat is a hunter",
         "The cat is hunting",
         "The man and the dog are hunters",
         "The man and the dog are hunting",
         "The man is a hunter",
         "The man is hunting"]

text_love = ["The people are lovers",
         "The people love each other",
         "The man and the woman are lovers",
         "The man and the woman love each other",
         "The girl and the boy are lovers",
         "The girl and the boy love each other",
         "The woman and the girl are lovers",
         "The woman and the girl love each other",
         "The man and the woman are lovers",
         "The man and the woman love each other"]

text_support = ["The woman is a supporter",
         "The woman is supporting",
         "The man is a supporter",
         "The man is supporting",
         "The people are supporters",
         "The people are supporting",
         "The man and the woman are supporters",
         "The man and the woman are supporting",
         "The hugging man is a supporter",
         "The hugging man is supporting"]

text_drive = ["The woman is a driver",
         "The woman is driving",
         "The person is a driver",
         "The person is driving",
         "The man is a driver",
         "The man is driving",
         "The woman with pink gloves is a driver",
         "The woman with pink gloves is driving"]

text_smoke = ["The man is a smoker",
         "The man is smoking",
         "The women are smokers",
         "The women are smoking",
         "The woman is a smoker",
         "The woman is smoking",
         "The man with the hat is a smoker",
         "The man with the hat is smoking"]

text_walk = ["The people are walkers",
         "The people are walking",
         "The girl is a walker",
         "The girl is walking"]

text_skii = ["The man is a skier",
         "The man is skiing",
         "The person is a skier",
         "The person is skiing",
         "The woman is a skier",
         "The woman is skiing",
         "The people are skiers",
         "The people are skiing"]

text_dance = ["The people are dancers",
         "The people are dancing",
         "The man and the girl are dancers",
         "The man and the girl are dancing",
         "The four women are dancers",
         "The four women are dancing",
         "The children are dancers",
         "The children are dancing"]

text_sing = ["The man is a singer",
         "The man is singing",
         "The woman is a singer",
         "The woman is singing",
         "The woman and the man are singers",
         "The woman and the man are singing"]

text_clean = ["The man is a cleaner",
         "The man is cleaning",
         "The woman is a cleaner",
         "The woman is cleaning",
         "The woman with the mop is a cleaner",
         "The woman with the mop is cleaning",
         "The two men are cleaners",
         "The two men are cleaning"]

text_game = ["The man at the computer is a gamer",
         "The man at the computer is gaming",
         "The two men are gamers",
         "The two men are gaming",
         "The child is a gamer",
         "The child is gaming",
         "The woman is a gamer",
         "The woman is gaming",
         "The people are gamers",
         "The people are gaming"]

text_teach = ["The adults are teachers",
         "The adults are teaching",
         "The woman is a teacher",
         "The woman is teaching",
         "The man is a teacher",
         "The man is teaching",
         "The two men are teachers",
         "The two men are teaching"]

text_paint = ["The person is a painter",
         "The person is painting",
         "The people are painters",
         "The people are painting",
         "The man is a painter",
         "The man is painting"]

text_read = ["The man is a reader",
         "The man is reading",
         "The woman is a reader",
         "The woman is reading",
         "The man and the woman are readers",
         "The man and the woman are reading",
         "The man and the child are readers",
         "The man and the child are reading",
         "The man in the blue jacket is a reader",
         "The man in the blue jacket is reading"]

text_swim = [ "The man is a swimmer",
         "The man is swimming",
         "The woman is a swimmer",
         "The woman is swimming",
         "The dog is a swimmer",
         "The dog is swimming",
         "The people are swimmers",
         "The people are swimming"]

text_run = ["The man in the black shirt is a runner",
         "The man in the black shirt is running",
         "The man is a runner",
         "The man is running",
         "The young man is a runner",
         "The young man is running",
         "The men are runners",
         "The men are running"]

from tqdm.notebook import tqdm
from PIL import Image
import urllib

scores_run = {}

for image_path in filename:
    if image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/run1_run1_g.6446422.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/run2_run2_g.4029465.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/run3_2403688.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/run4_2382318.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/Run5_1261.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/run6_2386888.jpg' :
        image_run = Image.open(image_path).convert("RGB")
        scores_run[image_path] = {}

        # run frcnn
        image_run, sizes, scales_yx = image_preprocess(image_path)
        output_dict = frcnn(image_run, sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=frcnn_cfg.max_detections,
            return_tensors="pt",
        )

        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")

        for text in tqdm(text_run):
              scores_run[image_path][text] = {}
              # encode
              # print(type(text))
              encoding = tokenizer(
                  text,
                  truncation=True,
                  return_token_type_ids=True,
                  return_attention_mask=True,
                  add_special_tokens=True,
                  return_tensors="pt"
              )

              outputs = model(
                           input_ids=encoding.input_ids,
                           attention_mask=encoding.attention_mask,
                           visual_feats=features,
                           visual_pos=normalized_boxes,
                           token_type_ids=encoding.token_type_ids,
                           return_dict=True,
                           output_attentions=False,
              )

              m = torch.nn.Softmax(dim=1)
              cross_score = m(outputs['cross_relationship_score'])
              scores_run[image_path][text]['cross_relationship_score'] = cross_score
scores_run

scores_swim = {}

for image_path in filename:
    if image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/swim1_g.6515320.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/swim2_3125002.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/swim3_2409365.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/swim4_2333478.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/Swim5_1251.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/swim6_ 2360491.jpg':

        image_swim = Image.open(image_path).convert("RGB")
        scores_swim[image_path] = {}

        # run frcnn
        image_swim, sizes, scales_yx = image_preprocess(image_path)
        output_dict = frcnn(image_swim, sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=frcnn_cfg.max_detections,
            return_tensors="pt",
        )

        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")

        for text in tqdm(text_swim):
              scores_swim[image_path][text] = {}
              # encode
              # print(type(text))
              encoding = tokenizer(
                  text,
                  truncation=True,
                  return_token_type_ids=True,
                  return_attention_mask=True,
                  add_special_tokens=True,
                  return_tensors="pt"
              )

              outputs = model(
                           input_ids=encoding.input_ids,
                           attention_mask=encoding.attention_mask,
                           visual_feats=features,
                           visual_pos=normalized_boxes,
                           token_type_ids=encoding.token_type_ids,
                           return_dict=True,
                           output_attentions=False,
              )

              m = torch.nn.Softmax(dim=1)
              #output = m(outputs['cross_relationship_score'])
              #output = output_lxmert['cross_relationship_score']
              cross_score = m(outputs['cross_relationship_score'])
              scores_swim[image_path][text]['cross_relationship_score'] = cross_score
scores_swim

scores_bake = {}

for image_path in filename:
    if image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/baker1_2351585.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/baker2_2362092.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/baker3_2403911.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/baker4_2365178.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/baker5_2391913.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/baker6_2360062.jpg':

        image_bake = Image.open(image_path).convert("RGB")
        scores_bake[image_path] = {}

        # run frcnn
        image_bake, sizes, scales_yx = image_preprocess(image_path)
        output_dict = frcnn(image_bake, sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=frcnn_cfg.max_detections,
            return_tensors="pt",
        )

        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")

        for text in tqdm(text_bake):
              scores_bake[image_path][text] = {}
              # encode
              # print(type(text))
              encoding = tokenizer(
                  text,
                  truncation=True,
                  return_token_type_ids=True,
                  return_attention_mask=True,
                  add_special_tokens=True,
                  return_tensors="pt"
              )

              outputs = model(
                              visual_feats=features,
                              visual_pos=output_dict.get("normalized_boxes"),
                              input_ids=encoding.input_ids,
                              attention_mask=encoding.attention_mask,
                              token_type_ids=encoding.token_type_ids,
                          )

              m = torch.nn.Softmax(dim=1)
              #output = m(outputs['cross_relationship_score'])
              #output = output_lxmert['cross_relationship_score']
              cross_score = m(outputs['cross_relationship_score'])
              scores_bake[image_path][text]['cross_relationship_score'] = cross_score
scores_bake

scores_surf = {}

for image_path in filename:
    if image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/surf1_000000465878.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/surf2_2379669.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/surf3_2319820.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/surf4_2346544.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/surf5_2396251.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/surf6_2371462.jpg':

        image_surf = Image.open(image_path).convert("RGB")
        scores_surf[image_path] = {}

        # run frcnn
        image_surf, sizes, scales_yx = image_preprocess(image_path)
        output_dict = frcnn(image_surf, sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=frcnn_cfg.max_detections,
            return_tensors="pt",
        )

        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")

        for text in tqdm(text_surf):
              scores_surf[image_path][text] = {}
              # encode
              # print(type(text))
              encoding = tokenizer(
                  text,
                  truncation=True,
                  return_token_type_ids=True,
                  return_attention_mask=True,
                  add_special_tokens=True,
                  return_tensors="pt"
              )

              outputs = model(
                              visual_feats=features,
                              visual_pos=output_dict.get("normalized_boxes"),
                              input_ids=encoding.input_ids,
                              attention_mask=encoding.attention_mask,
                              token_type_ids=encoding.token_type_ids,
                          )

              m = torch.nn.Softmax(dim=1)
              #output = m(outputs['cross_relationship_score'])
              #output = output_lxmert['cross_relationship_score']
              cross_score = m(outputs['cross_relationship_score'])
              scores_surf[image_path][text]['cross_relationship_score'] = cross_score
scores_surf

scores_hunt = {}

for image_path in filename:
    if image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/hunt1_2316137.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/hunt2_Wiki.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/hunt3_ 2316613.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/hunt4_Wiki.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/hunt5_2337201.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/hunter6_Wiki.jpg':

        image_hunt = Image.open(image_path).convert("RGB")
        scores_hunt[image_path] = {}

        # run frcnn
        image_hunt, sizes, scales_yx = image_preprocess(image_path)
        output_dict = frcnn(image_hunt, sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=frcnn_cfg.max_detections,
            return_tensors="pt",
        )

        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")

        for text in tqdm(text_hunt):
              scores_hunt[image_path][text] = {}
              # encode
              # print(type(text))
              encoding = tokenizer(
                  text,
                  truncation=True,
                  return_token_type_ids=True,
                  return_attention_mask=True,
                  add_special_tokens=True,
                  return_tensors="pt"
              )

              outputs = model(
                              visual_feats=features,
                              visual_pos=output_dict.get("normalized_boxes"),
                              input_ids=encoding.input_ids,
                              attention_mask=encoding.attention_mask,
                              token_type_ids=encoding.token_type_ids,
                          )

              m = torch.nn.Softmax(dim=1)
              #output = m(outputs['cross_relationship_score'])
              #output = output_lxmert['cross_relationship_score']
              cross_score = m(outputs['cross_relationship_score'])
              scores_hunt[image_path][text]['cross_relationship_score'] = cross_score
scores_hunt

scores_love = {}

for image_path in filename:
    if image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/Love1_2357211.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/love2_2393418.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/love3_2361720.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/love4_2368165.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/love5_2373402.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/love6_2373244.jpg':

        image_love = Image.open(image_path).convert("RGB")
        scores_love[image_path] = {}

        # run frcnn
        image_love, sizes, scales_yx = image_preprocess(image_path)
        output_dict = frcnn(image_love, sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=frcnn_cfg.max_detections,
            return_tensors="pt",
        )

        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")

        for text in tqdm(text_love):
              scores_love[image_path][text] = {}
              # encode
              # print(type(text))
              encoding = tokenizer(
                  text,
                  truncation=True,
                  return_token_type_ids=True,
                  return_attention_mask=True,
                  add_special_tokens=True,
                  return_tensors="pt"
              )

              outputs = model(
                              visual_feats=features,
                              visual_pos=output_dict.get("normalized_boxes"),
                              input_ids=encoding.input_ids,
                              attention_mask=encoding.attention_mask,
                              token_type_ids=encoding.token_type_ids,
                          )

              m = torch.nn.Softmax(dim=1)
              #output = m(outputs['cross_relationship_score'])
              #output = output_lxmert['cross_relationship_score']
              cross_score = m(outputs['cross_relationship_score'])
              scores_love[image_path][text]['cross_relationship_score'] = cross_score
scores_love

scores_support = {}

for image_path in filename:
    if image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/support1_2349575.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/support2_2407009.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/support3_2324470.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/support4_2346519.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/support5_2319026.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/support6_2345151.jpg':

        image_support = Image.open(image_path).convert("RGB")
        scores_support[image_path] = {}

        # run frcnn
        image_support, sizes, scales_yx = image_preprocess(image_path)
        output_dict = frcnn(image_support, sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=frcnn_cfg.max_detections,
            return_tensors="pt",
        )

        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")

        for text in tqdm(text_support):
              scores_support[image_path][text] = {}
              # encode
              # print(type(text))
              encoding = tokenizer(
                  text,
                  truncation=True,
                  return_token_type_ids=True,
                  return_attention_mask=True,
                  add_special_tokens=True,
                  return_tensors="pt"
              )

              outputs = model(
                              visual_feats=features,
                              visual_pos=output_dict.get("normalized_boxes"),
                              input_ids=encoding.input_ids,
                              attention_mask=encoding.attention_mask,
                              token_type_ids=encoding.token_type_ids,
                          )

              m = torch.nn.Softmax(dim=1)
              #output = m(outputs['cross_relationship_score'])
              #output = output_lxmert['cross_relationship_score']
              cross_score = m(outputs['cross_relationship_score'])
              scores_support[image_path][text]['cross_relationship_score'] = cross_score
scores_support

scores_drive = {}

for image_path in filename:
    if image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/drive1_Wiki.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/drive2_Wiki.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/drive3_Wiki.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/drive4_2379270.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/drive5_wikipidia.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/drive6_3608.jpg':

        image_drive = Image.open(image_path).convert("RGB")
        scores_drive[image_path] = {}

        # run frcnn
        image_drive, sizes, scales_yx = image_preprocess(image_path)
        output_dict = frcnn(image_drive, sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=frcnn_cfg.max_detections,
            return_tensors="pt",
        )

        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")

        for text in tqdm(text_drive):
              scores_drive[image_path][text] = {}
              # encode
              # print(type(text))
              encoding = tokenizer(
                  text,
                  truncation=True,
                  return_token_type_ids=True,
                  return_attention_mask=True,
                  add_special_tokens=True,
                  return_tensors="pt"
              )

              outputs = model(
                              visual_feats=features,
                              visual_pos=output_dict.get("normalized_boxes"),
                              input_ids=encoding.input_ids,
                              attention_mask=encoding.attention_mask,
                              token_type_ids=encoding.token_type_ids,
                          )

              m = torch.nn.Softmax(dim=1)
              #output = m(outputs['cross_relationship_score'])
              #output = output_lxmert['cross_relationship_score']
              cross_score = m(outputs['cross_relationship_score'])
              scores_drive[image_path][text]['cross_relationship_score'] = cross_score
scores_drive

scores_smoke = {}

for image_path in filename:
    if image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/smoking1_2414691.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/smoke2_wiki.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/smoking3_2411117.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/smoke4_wiki.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/smoking5_2358939.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/smoke6_wiki.jpg':

        image_smoke = Image.open(image_path).convert("RGB")
        scores_smoke[image_path] = {}

        # run frcnn
        image_smoke, sizes, scales_yx = image_preprocess(image_path)
        output_dict = frcnn(image_smoke, sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=frcnn_cfg.max_detections,
            return_tensors="pt",
        )

        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")

        for text in tqdm(text_smoke):
              scores_smoke[image_path][text] = {}
              # encode
              # print(type(text))
              encoding = tokenizer(
                  text,
                  truncation=True,
                  return_token_type_ids=True,
                  return_attention_mask=True,
                  add_special_tokens=True,
                  return_tensors="pt"
              )

              outputs = model(
                              visual_feats=features,
                              visual_pos=output_dict.get("normalized_boxes"),
                              input_ids=encoding.input_ids,
                              attention_mask=encoding.attention_mask,
                              token_type_ids=encoding.token_type_ids,
                          )

              m = torch.nn.Softmax(dim=1)
              #output = m(outputs['cross_relationship_score'])
              #output = output_lxmert['cross_relationship_score']
              cross_score = m(outputs['cross_relationship_score'])
              scores_smoke[image_path][text]['cross_relationship_score'] = cross_score
scores_smoke

scores_walk = {}

for image_path in filename:
    if image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/walk1_g.4301822.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/walk2_g.6852045.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/Walk3_g.6768117.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/walk4_g.6729783.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/walk5_g.5494503.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/walk6_g.5392781.jpg':

        image_walk = Image.open(image_path).convert("RGB")
        scores_walk[image_path] = {}

        # run frcnn
        image_walk, sizes, scales_yx = image_preprocess(image_path)
        output_dict = frcnn(image_walk, sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=frcnn_cfg.max_detections,
            return_tensors="pt",
        )

        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")

        for text in tqdm(text_walk):
              scores_walk[image_path][text] = {}
              # encode
              # print(type(text))
              encoding = tokenizer(
                  text,
                  truncation=True,
                  return_token_type_ids=True,
                  return_attention_mask=True,
                  add_special_tokens=True,
                  return_tensors="pt"
              )

              outputs = model(
                              visual_feats=features,
                              visual_pos=output_dict.get("normalized_boxes"),
                              input_ids=encoding.input_ids,
                              attention_mask=encoding.attention_mask,
                              token_type_ids=encoding.token_type_ids,
                          )

              m = torch.nn.Softmax(dim=1)
              #output = m(outputs['cross_relationship_score'])
              #output = output_lxmert['cross_relationship_score']
              cross_score = m(outputs['cross_relationship_score'])
              scores_walk[image_path][text]['cross_relationship_score'] = cross_score
scores_walk

scores_skii = {}

for image_path in filename:
    if image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/skier1_2354995.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/skiing2_2373807.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/skiing3_2406324.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/skier4_2328749.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/skier5_000000186254.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/skiing6_g.2276706.jpg':

        image_skii = Image.open(image_path).convert("RGB")
        scores_skii[image_path] = {}

        # run frcnn
        image_skii, sizes, scales_yx = image_preprocess(image_path)
        output_dict = frcnn(image_skii, sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=frcnn_cfg.max_detections,
            return_tensors="pt",
        )

        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")

        for text in tqdm(text_skii):
              scores_skii[image_path][text] = {}
              # encode
              # print(type(text))
              encoding = tokenizer(
                  text,
                  truncation=True,
                  return_token_type_ids=True,
                  return_attention_mask=True,
                  add_special_tokens=True,
                  return_tensors="pt"
              )

              outputs = model(
                              visual_feats=features,
                              visual_pos=output_dict.get("normalized_boxes"),
                              input_ids=encoding.input_ids,
                              attention_mask=encoding.attention_mask,
                              token_type_ids=encoding.token_type_ids,
                          )

              m = torch.nn.Softmax(dim=1)
              #output = m(outputs['cross_relationship_score'])
              #output = output_lxmert['cross_relationship_score']
              cross_score = m(outputs['cross_relationship_score'])
              scores_skii[image_path][text]['cross_relationship_score'] = cross_score
scores_skii

scores_dance = {}

for image_path in filename:
    if image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/dance1_2413339.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/dance2_2348207.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/dance3_2405931.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/dance4_2322738.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/dance5_2364650.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/dance6_g.3295466.jpg':

        image_dance = Image.open(image_path).convert("RGB")
        scores_dance[image_path] = {}

        # run frcnn
        image_dance, sizes, scales_yx = image_preprocess(image_path)
        output_dict = frcnn(image_dance, sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=frcnn_cfg.max_detections,
            return_tensors="pt",
        )

        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")

        for text in tqdm(text_dance):
              scores_dance[image_path][text] = {}
              # encode
              # print(type(text))
              encoding = tokenizer(
                  text,
                  truncation=True,
                  return_token_type_ids=True,
                  return_attention_mask=True,
                  add_special_tokens=True,
                  return_tensors="pt"
              )

              outputs = model(
                              visual_feats=features,
                              visual_pos=output_dict.get("normalized_boxes"),
                              input_ids=encoding.input_ids,
                              attention_mask=encoding.attention_mask,
                              token_type_ids=encoding.token_type_ids,
                          )

              m = torch.nn.Softmax(dim=1)
              #output = m(outputs['cross_relationship_score'])
              #output = output_lxmert['cross_relationship_score']
              cross_score = m(outputs['cross_relationship_score'])
              scores_dance[image_path][text]['cross_relationship_score'] = cross_score
scores_dance

scores_sing = {}

for image_path in filename:
    if image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/singer1_2324539.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/singer2_g.7140513.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/singer3_2415610.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/singer4_2348041.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/singer5_2341426.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/singer6_2390352.jpg':

        image_sing = Image.open(image_path).convert("RGB")
        scores_sing[image_path] = {}

        # run frcnn
        image_sing, sizes, scales_yx = image_preprocess(image_path)
        output_dict = frcnn(image_sing, sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=frcnn_cfg.max_detections,
            return_tensors="pt",
        )

        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")

        for text in tqdm(text_sing):
              scores_sing[image_path][text] = {}
              # encode
              # print(type(text))
              encoding = tokenizer(
                  text,
                  truncation=True,
                  return_token_type_ids=True,
                  return_attention_mask=True,
                  add_special_tokens=True,
                  return_tensors="pt"
              )

              outputs = model(
                              visual_feats=features,
                              visual_pos=output_dict.get("normalized_boxes"),
                              input_ids=encoding.input_ids,
                              attention_mask=encoding.attention_mask,
                              token_type_ids=encoding.token_type_ids,
                          )

              m = torch.nn.Softmax(dim=1)
              #output = m(outputs['cross_relationship_score'])
              #output = output_lxmert['cross_relationship_score']
              cross_score = m(outputs['cross_relationship_score'])
              scores_sing[image_path][text]['cross_relationship_score'] = cross_score
scores_sing

scores_clean = {}

for image_path in filename:
    if image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/cleaner1_2318276.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/clean2_Wiki.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/cleaner3_2413372.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/clean4_g.6779564.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/clean5_Wiki.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/cleaner6_2366810.jpg':

        image_clean = Image.open(image_path).convert("RGB")
        scores_clean[image_path] = {}

        # run frcnn
        image_clean, sizes, scales_yx = image_preprocess(image_path)
        output_dict = frcnn(image_clean, sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=frcnn_cfg.max_detections,
            return_tensors="pt",
        )

        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")

        for text in tqdm(text_clean):
              scores_clean[image_path][text] = {}
              # encode
              # print(type(text))
              encoding = tokenizer(
                  text,
                  truncation=True,
                  return_token_type_ids=True,
                  return_attention_mask=True,
                  add_special_tokens=True,
                  return_tensors="pt"
              )

              outputs = model(
                              visual_feats=features,
                              visual_pos=output_dict.get("normalized_boxes"),
                              input_ids=encoding.input_ids,
                              attention_mask=encoding.attention_mask,
                              token_type_ids=encoding.token_type_ids,
                          )

              m = torch.nn.Softmax(dim=1)
              #output = m(outputs['cross_relationship_score'])
              #output = output_lxmert['cross_relationship_score']
              cross_score = m(outputs['cross_relationship_score'])
              scores_clean[image_path][text]['cross_relationship_score'] = cross_score
scores_clean

scores_game = {}

for image_path in filename:
    if image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/gaming1_000000017967.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/game2_Wiki.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/game3_Wiki.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/gaming4_2383083.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/gaming5_2349567.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/gaming6_2396868.jpg':

        image_game = Image.open(image_path).convert("RGB")
        scores_game[image_path] = {}

        # run frcnn
        image_game, sizes, scales_yx = image_preprocess(image_path)
        output_dict = frcnn(image_game, sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=frcnn_cfg.max_detections,
            return_tensors="pt",
        )

        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")

        for text in tqdm(text_game):
              scores_game[image_path][text] = {}
              # encode
              # print(type(text))
              encoding = tokenizer(
                  text,
                  truncation=True,
                  return_token_type_ids=True,
                  return_attention_mask=True,
                  add_special_tokens=True,
                  return_tensors="pt"
              )

              outputs = model(
                              visual_feats=features,
                              visual_pos=output_dict.get("normalized_boxes"),
                              input_ids=encoding.input_ids,
                              attention_mask=encoding.attention_mask,
                              token_type_ids=encoding.token_type_ids,
                          )

              m = torch.nn.Softmax(dim=1)
              #output = m(outputs['cross_relationship_score'])
              #output = output_lxmert['cross_relationship_score']
              cross_score = m(outputs['cross_relationship_score'])
              scores_game[image_path][text]['cross_relationship_score'] = cross_score
scores_game

scores_teach = {}

for image_path in filename:
    if image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/teacher1_2323360.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/teacher2_2361701.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/teacher3_2400586.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/teacher4_2394237.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/teacher5_2391875.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/teacher6_2408090.jpg':

        image_teach = Image.open(image_path).convert("RGB")
        scores_teach[image_path] = {}

        # run frcnn
        image_teach, sizes, scales_yx = image_preprocess(image_path)
        output_dict = frcnn(image_teach, sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=frcnn_cfg.max_detections,
            return_tensors="pt",
        )

        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")

        for text in tqdm(text_teach):
              scores_teach[image_path][text] = {}
              # encode
              # print(type(text))
              encoding = tokenizer(
                  text,
                  truncation=True,
                  return_token_type_ids=True,
                  return_attention_mask=True,
                  add_special_tokens=True,
                  return_tensors="pt"
              )

              outputs = model(
                              visual_feats=features,
                              visual_pos=output_dict.get("normalized_boxes"),
                              input_ids=encoding.input_ids,
                              attention_mask=encoding.attention_mask,
                              token_type_ids=encoding.token_type_ids,
                          )

              m = torch.nn.Softmax(dim=1)
              #output = m(outputs['cross_relationship_score'])
              #output = output_lxmert['cross_relationship_score']
              cross_score = m(outputs['cross_relationship_score'])
              scores_teach[image_path][text]['cross_relationship_score'] = cross_score
scores_teach

scores_paint = {}

for image_path in filename:
    if image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/Paint1_g.5813326.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/painting2_Paint2_g.4061871.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/painting3_Wiki.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/painting4_paint4_g.6141535.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/painting5_1021.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/painting6_Wiki.jpg':

        image_paint = Image.open(image_path).convert("RGB")
        scores_paint[image_path] = {}

        # run frcnn
        image_paint, sizes, scales_yx = image_preprocess(image_path)
        output_dict = frcnn(image_paint, sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=frcnn_cfg.max_detections,
            return_tensors="pt",
        )

        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")

        for text in tqdm(text_paint):
              scores_paint[image_path][text] = {}
              # encode
              # print(type(text))
              encoding = tokenizer(
                  text,
                  truncation=True,
                  return_token_type_ids=True,
                  return_attention_mask=True,
                  add_special_tokens=True,
                  return_tensors="pt"
              )

              outputs = model(
                              visual_feats=features,
                              visual_pos=output_dict.get("normalized_boxes"),
                              input_ids=encoding.input_ids,
                              attention_mask=encoding.attention_mask,
                              token_type_ids=encoding.token_type_ids,
                          )

              m = torch.nn.Softmax(dim=1)
              #output = m(outputs['cross_relationship_score'])
              #output = output_lxmert['cross_relationship_score']
              cross_score = m(outputs['cross_relationship_score'])
              scores_paint[image_path][text]['cross_relationship_score'] = cross_score
scores_paint

scores_read = {}

for image_path in filename:
    if image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/read1_Wiki.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/reading2_2397070.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/read3_Wiki.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/reading4_wiki.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/reading5_2337870.jpg' or \
    image_path == '/content/gdrive/MyDrive/DRIVE_final_target_images/reading6_2362825.jpg':

        image_read = Image.open(image_path).convert("RGB")
        scores_read[image_path] = {}

        # run frcnn
        image_read, sizes, scales_yx = image_preprocess(image_path)
        output_dict = frcnn(image_read, sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=frcnn_cfg.max_detections,
            return_tensors="pt",
        )

        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")

        for text in tqdm(text_read):
              scores_read[image_path][text] = {}
              # encode
              # print(type(text))
              encoding = tokenizer(
                  text,
                  truncation=True,
                  return_token_type_ids=True,
                  return_attention_mask=True,
                  add_special_tokens=True,
                  return_tensors="pt"
              )

              outputs = model(
                              visual_feats=features,
                              visual_pos=output_dict.get("normalized_boxes"),
                              input_ids=encoding.input_ids,
                              attention_mask=encoding.attention_mask,
                              token_type_ids=encoding.token_type_ids,
                          )

              m = torch.nn.Softmax(dim=1)
              #output = m(outputs['cross_relationship_score'])
              #output = output_lxmert['cross_relationship_score']
              cross_score = m(outputs['cross_relationship_score'])
              scores_read[image_path][text]['cross_relationship_score'] = cross_score
scores_read
